{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "from pprint import pprint\n",
    "\n",
    "from src.LoraCreator import LORAFactory\n",
    "\n",
    "from peft import PeftModel\n",
    "from IPython.display import display\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "foundation_model_path = \"/home/ec2-user/Llama-2-13B-Chat-fp16\"\n",
    "finetuned_model_path = \"/home/ec2-user/merged_llama2_by_files\"\n",
    "adapter_model_path = \"/home/ec2-user/nghi/llama2_lora_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_factory = LORAFactory(foundation_model_path, finetuned_model_path, adapter_model_path, rank=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading foundational model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7269a18d71834dcba75b4be3d8993b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading fine-tuned model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37743294a554967bc0b44298c97dd93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize adapter ...\n"
     ]
    }
   ],
   "source": [
    "lora_factory.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing low rank matrix for model.layers.0.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.0.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.1.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.1.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.2.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.2.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.3.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.3.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.4.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.4.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.5.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.5.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.6.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.6.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.7.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.7.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.8.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.8.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.9.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.9.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.10.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.10.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.11.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.11.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.12.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.12.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.13.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.13.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.14.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.14.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.15.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.15.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.16.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.16.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.17.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.17.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.18.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.18.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.19.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.19.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.20.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.20.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.21.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.21.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.22.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.22.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.23.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.23.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.24.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.24.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.25.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.25.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.26.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.26.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.27.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.27.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.28.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.28.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.29.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.29.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.30.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.30.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.31.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.31.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.32.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.32.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.33.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.33.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.34.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.34.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.35.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.35.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.36.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.36.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.37.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.37.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.38.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.38.self_attn.v_proj ...\n",
      "computing low rank matrix for model.layers.39.self_attn.q_proj ...\n",
      "computing low rank matrix for model.layers.39.self_attn.v_proj ...\n"
     ]
    }
   ],
   "source": [
    "lora_factory.create_adapter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_lora_model = PeftModel.from_pretrained(\n",
    "        lora_factory.foundation_model,\n",
    "        lora_factory.adapter_model_path,\n",
    "        # torch_dtype=torch.float16\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying the LoRA\n"
     ]
    }
   ],
   "source": [
    "merged_lora_model_path = \"/home/ec2-user/nghi/merged_llama2_by_files_LORA\"\n",
    "print(\"Applying the LoRA\")\n",
    "new_merged_model = merged_lora_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tokenizer = AutoTokenizer.from_pretrained(lora_factory.foundation_model_path, use_fast=False, legacy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/ec2-user/nghi/merged_llama2_by_files_LORA/tokenizer_config.json',\n",
       " '/home/ec2-user/nghi/merged_llama2_by_files_LORA/special_tokens_map.json',\n",
       " '/home/ec2-user/nghi/merged_llama2_by_files_LORA/tokenizer.model',\n",
       " '/home/ec2-user/nghi/merged_llama2_by_files_LORA/added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_merged_model.save_pretrained(merged_lora_model_path)\n",
    "base_tokenizer.save_pretrained(merged_lora_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastchat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
